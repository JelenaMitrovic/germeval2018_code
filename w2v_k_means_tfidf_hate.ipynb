{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################# import libraries ###########################################\n",
    "%load_ext jupyternotify\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset = pd.DataFrame(columns=['X', 'y1', 'y2'])\n",
    "    #print('Loading dataset...')\n",
    "    with codecs.open(path, \"r\", encoding='utf-8', errors='ignore') as fdata:\n",
    "        for line in tqdm(fdata.readlines()):\n",
    "            line_split = line.split()\n",
    "            formated = ' '.join(line_split[:-2])\n",
    "            dataset.loc[-1] = [formated, line_split[-2], line_split[-1]]  # adding a row\n",
    "            dataset.index = dataset.index + 1  # shifting index\n",
    "            dataset = dataset.sort_index()  # sorting by index\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_ext(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.drop(columns=['count'])\n",
    "    df_droped = df.drop(columns=['count', 'hate_speech', 'offensive_language', 'neither'])\n",
    "    df_droped.columns = ['y', 'X']\n",
    "    return df_droped.replace([0, 1], 'OFFENSE').replace(2, 'OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset_hate(path):\n",
    "    dataset = pd.DataFrame(columns=['X', 'y1', 'y2', 'rating'])\n",
    "    with codecs.open(path, \"r\", encoding='utf-8', errors='ignore') as fdata:\n",
    "        for line in tqdm(fdata.readlines()):\n",
    "            line = line.strip()\n",
    "            line_split = line.split(';')\n",
    "            formated = ' '.join(line_split[:-3])\n",
    "            dataset.loc[-1] = [formated, line_split[-3], line_split[-2], line_split[-1]]  # adding a row\n",
    "            dataset.index = dataset.index + 1  # shifting index\n",
    "            dataset = dataset.sort_index()  # sorting by index\n",
    "    return dataset.drop(dataset.index[[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "\n",
    "usernamePattern = re.compile('@[A-Za-z0-9_]{1,15}')\n",
    "urlPattern = re.compile('(https?:\\/\\/)[\\/.:\\w(1-9)]*\\s?')\n",
    "lbrPattern = re.compile('|LBR|')\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "tkz = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(usernamePattern, \"\",  text)\n",
    "    text = re.sub(urlPattern, \"\",  text)\n",
    "    text = re.sub(lbrPattern, \"\",  text)\n",
    "    output = []\n",
    "    tokens = tkz.tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_german:\n",
    "            if len(token) > 1:\n",
    "                if token[0] == '#':\n",
    "                    token = token[1:]\n",
    "                output.append(token)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tfidf(text):\n",
    "    text = re.sub(usernamePattern, \"\",  text)\n",
    "    text = re.sub(urlPattern, \"\",  text)\n",
    "    text = re.sub(lbrPattern, \"\",  text)\n",
    "    output = []\n",
    "    tokens = tkz.tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_german:\n",
    "            if len(token) > 1:\n",
    "                if token[0] == '#':\n",
    "                    token = token[1:]\n",
    "                output.append(stemmer.stem(token))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_ext(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.drop(columns=['count'])\n",
    "    df_droped = df.drop(columns=['count', 'hate_speech', 'offensive_language', 'neither'])\n",
    "    df_droped.columns = ['y1', 'X']\n",
    "    return df_droped.replace([0, 1], 'OFFENSE').replace(2, 'OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5009/5009 [00:15<00:00, 333.38it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(path = '/home/text_mining_project/text_mining_project_2018/evaluation/germeval2018.training.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3189/3189 [00:09<00:00, 350.55it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_own = load_dataset(path = '/home/text_mining_project/export.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_david = load_dataset_ext('/home/text_mining_project/t-davidson.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 470/470 [00:01<00:00, 349.75it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_hate = load_dataset_hate('/home/text_mining_project/text_mining_project_2018/evaluation/german_hatespeech_refugees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_full =  pd.concat([dataset, dataset_own])\n",
    "# dataset_full = dataset_david\n",
    "dataset_full = dataset_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_bin(y, predicted_label):\n",
    "    choose = lambda l : True if l == predicted_label else False\n",
    "    return [choose(l) for l in y]\n",
    "\n",
    "y1 = np.array(encode_label_bin(dataset_hate['y1'], 'YES'))\n",
    "y2 = np.array(encode_label_bin(dataset_hate['y2'], 'YES'))\n",
    "\n",
    "y = (y1 | y2)*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "X = dataset_full['X'].values\n",
    "# y = dataset_full['y1'].values\n",
    "\n",
    "# lb = LabelEncoder()\n",
    "# lb.fit(['OFFENSE','OTHER'])\n",
    "# y = np.ones(len(y)) - lb.transform(y)\n",
    "\n",
    "X_train_dataset, X_test_dataset, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming documents...\n",
      "Transformation finished!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features = None\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "# stopwords_german = set(stopwords.words('english'))\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_tfidf, stop_words=stopwords_german, max_features=max_features, ngram_range=(1,3))\n",
    "print('Transforming documents...')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_dataset)\n",
    "X_test_tfidf = tfidf.transform(X_test_dataset)\n",
    "print('Transformation finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_nostem_stopword.model\")\n",
    "# model = Word2Vec.load(\"word2vec_nostem_stopword_english.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "def make_bag_of_centroids(X, word_centroid_map, cluster_size):\n",
    "    centroids_list = []\n",
    "    for sentence in X:\n",
    "        centroids = zeros(cluster_size, dtype=\"float32\")\n",
    "        for word in sentence:\n",
    "            if word in word_centroid_map:\n",
    "                centroids[word_centroid_map[word]] += 1\n",
    "        centroids_list.append(centroids)\n",
    "    return centroids_list\n",
    "\n",
    "as_centroid = lambda s: make_bag_of_centroids(s, word2centroid, kmeans_args['n_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koogleschreiber/.local/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning:\n",
      "\n",
      "init_size=300 should be larger than k=1000. Setting it to 3*k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans_args = {\n",
    "    'n_clusters': 1000,\n",
    "}\n",
    "\n",
    "# clustering = KMeans(**kmeans_args).fit_predict(model.wv.vectors)\n",
    "\n",
    "clustering = MiniBatchKMeans(**kmeans_args).fit_predict(model.wv.vectors)\n",
    "pickle.dump(clustering, open('./minibatchkmeans.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2centroid = {k: v for k, v in zip(model.wv.index2word, clustering)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vector_lists(x1, x2):\n",
    "    result = []\n",
    "    for i in range(len(x1)):\n",
    "        result.append(x1[i] + x2[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_german = set(stopwords.words('german'))\n",
    "# stopwords_german = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(X):\n",
    "    return [tokenize(sentence) for sentence in X]\n",
    "\n",
    "X_preprocess_train = preprocess(X_train_dataset)\n",
    "X_preprocess_test = preprocess(X_test_dataset)\n",
    "\n",
    "X_train_centroid = scale(as_centroid(X_preprocess_train))\n",
    "X_test_centroid = scale(as_centroid(X_preprocess_test))\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "X_train_centroid_sparse = csr_matrix(X_train_centroid)\n",
    "X_test_centroid_sparse = csr_matrix(X_test_centroid)\n",
    "\n",
    "\n",
    "X_train = hstack([X_train_centroid_sparse, X_train_tfidf])\n",
    "X_test = hstack([X_test_centroid_sparse, X_test_tfidf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lsvc = LinearSVC(penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_train = model.transform(X_train)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2\n",
    "# \n",
    "# scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "# X_train = scaler.fit_transform(X_train.todense())\n",
    "# X_test = scaler.transform(X_test.todense())\n",
    "# \n",
    "# ch2 = SelectKBest(chi2, k=2000)\n",
    "# X_train = ch2.fit_transform(X_train, y_train)\n",
    "# X_test = ch2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "seed = 42\n",
    "k = 7\n",
    "jobs = -1\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best estimator...\n",
      "\n",
      "Mean accuracy Linear SVM: 0.658 (+/- 0.149)\n",
      "Mean accuracy SGDClassifier: 0.711 (+/- 0.093)\n",
      "Mean accuracy BernoulliNB: 0.720 (+/- 0.088)\n",
      "Mean accuracy LogisticRegression: 0.692 (+/- 0.124)\n",
      "Mean accuracy KNeighborsClassifier: 0.739 (+/- 0.055)\n",
      "Mean accuracy AdaBoostClassifier: 0.715 (+/- 0.105)\n",
      "Mean accuracy Random Forest: 0.742 (+/- 0.069)\n",
      "Mean accuracy Decision Tree: 0.720 (+/- 0.032)\n",
      "\n",
      "Best estimator: Random Forest (mean acc 0.742, 7-fold cross-validation)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "names = [\"Linear SVM\", \"SGDClassifier\", \"BernoulliNB\", \"LogisticRegression\",\n",
    "         \"KNeighborsClassifier\", \"AdaBoostClassifier\", \"Random Forest\", \"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    LinearSVC(random_state=seed),\n",
    "    SGDClassifier(max_iter=1000, tol=None),\n",
    "    BernoulliNB(),\n",
    "    LogisticRegression(random_state=seed, solver='sag', max_iter=1000),\n",
    "    KNeighborsClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    RandomForestClassifier(random_state=seed),\n",
    "    DecisionTreeClassifier(random_state=seed)\n",
    "]\n",
    "\n",
    "print('Searching best estimator...')\n",
    "print()\n",
    "best_classifier = None\n",
    "for name, clf in zip(names, classifiers):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=k, n_jobs=jobs)\n",
    "    print('Mean accuracy %s: %0.3f (+/- %0.3f)' % (name, scores.mean(), scores.std() * 2))\n",
    "    if not best_classifier:\n",
    "        best_classifier = (name, scores.mean())\n",
    "    else:\n",
    "        if best_classifier[1] < scores.mean():\n",
    "            best_classifier = (name, scores.mean())\n",
    "print()\n",
    "print('Best estimator: %s (mean acc %0.3f, %d-fold cross-validation)' % (best_classifier[0], best_classifier[1], k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best estimator (F1 score) ...\n",
      "\n",
      "Mean F1 score Linear SVM: 0.542 (+/- 0.200)\n",
      "Mean F1 score SGDClassifier: 0.396 (+/- 0.230)\n",
      "Mean F1 score BernoulliNB: 0.492 (+/- 0.222)\n",
      "Mean F1 score LogisticRegression: 0.636 (+/- 0.139)\n",
      "Mean F1 score KNeighborsClassifier: 0.458 (+/- 0.146)\n",
      "Mean F1 score AdaBoostClassifier: 0.527 (+/- 0.196)\n",
      "Mean F1 score Random Forest: 0.518 (+/- 0.120)\n",
      "Mean F1 score Decision Tree: 0.553 (+/- 0.093)\n",
      "\n",
      "Best estimator: LogisticRegression (mean F1 score 0.636, 7-fold cross-validation)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "print('Searching best estimator (F1 score) ...')\n",
    "print()\n",
    "best_classifier = None\n",
    "for name, clf in zip(names, classifiers):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=k, n_jobs=jobs, scoring='f1')\n",
    "    print('Mean F1 score %s: %0.3f (+/- %0.3f)' % (name, scores.mean(), scores.std() * 2))\n",
    "    if not best_classifier:\n",
    "        best_classifier = (name, scores.mean())\n",
    "    else:\n",
    "        if best_classifier[1] < scores.mean():\n",
    "            best_classifier = (name, scores.mean())\n",
    "print()\n",
    "print('Best estimator: %s (mean F1 score %0.3f, %d-fold cross-validation)' % (best_classifier[0], best_classifier[1], k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score predictions:  0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1=f1_score(predictions, y_test)\n",
    "print(\"F1-Score predictions: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB(alpha=0.1, binarize=0, class_prior=None, fit_prior=False)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score predictions:  0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1=f1_score(predictions, y_test)\n",
    "print(\"F1-Score predictions: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score predictions:  0.5384615384615385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1=f1_score(predictions, y_test)\n",
    "print(\"F1-Score predictions: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
