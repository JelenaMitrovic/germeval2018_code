{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################# import libraries ###########################################\n",
    "%load_ext jupyternotify\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset = pd.DataFrame(columns=['X', 'y1', 'y2'])\n",
    "    #print('Loading dataset...')\n",
    "    with codecs.open(path, \"r\", encoding='utf-8', errors='ignore') as fdata:\n",
    "        for line in tqdm(fdata.readlines()):\n",
    "            line_split = line.split()\n",
    "            formated = ' '.join(line_split[:-2])\n",
    "            dataset.loc[-1] = [formated, line_split[-2], line_split[-1]]  # adding a row\n",
    "            dataset.index = dataset.index + 1  # shifting index\n",
    "            dataset = dataset.sort_index()  # sorting by index\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text):\n",
    "    tkz = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    stemmer = SnowballStemmer(\"german\")\n",
    "    output = []\n",
    "    tokens = tkz.tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_german:\n",
    "            if len(token) > 1:\n",
    "                if token[0] == '#':\n",
    "                    token = token[1:]\n",
    "                output.append(token)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5009/5009 [00:14<00:00, 346.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(path = '/home/text_mining_project/text_mining_project_2018/evaluation/germeval2018.training.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "X = dataset['X'].values\n",
    "y = dataset['y1'].values\n",
    "\n",
    "lb = LabelEncoder()\n",
    "lb.fit(['OFFENSE','OTHER'])\n",
    "y = np.ones(len(y)) - lb.transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_german = set(stopwords.words('german'))\n",
    "\n",
    "def preprocess(X):\n",
    "    return [tokenize(sentence) for sentence in X]\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2vmodel(path):\n",
    "    with open(path, \"rb\") as lines:\n",
    "        w2v = {line.split()[0]: np.array(list(map(float, line.split()[1:])))\n",
    "                for line in tqdm(lines)}\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KeyedVectors.load_word2vec_format(datapath(\"/home/text_mining_project/twitter-de_d100_w5_min10.bin\"), binary=True)\n",
    "model = Word2Vec.load(\"word2vec_nostem.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fleig', 0.47227969765663147),\n",
       " ('sonderbedeutung', 0.4633481502532959),\n",
       " ('vorwirft', 0.4559037387371063),\n",
       " ('red-pack', 0.4509229063987732),\n",
       " ('sparxsystems', 0.4493067264556885),\n",
       " ('sachggegenstand', 0.44587182998657227),\n",
       " ('multiversum', 0.44581305980682373),\n",
       " ('dazugehöre', 0.44405651092529297),\n",
       " ('blaumacher', 0.44281986355781555),\n",
       " ('gegenüb', 0.4426388144493103)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('untermensch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5590099346449514"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('afd', 'gauland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koogleschreiber/.local/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning:\n",
      "\n",
      "init_size=300 should be larger than k=1000. Setting it to 3*k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans_args = {\n",
    "    'n_clusters': 1000,\n",
    "}\n",
    "\n",
    "# clustering = KMeans(**kmeans_args).fit_predict(model.wv.vectors)\n",
    "\n",
    "clustering = MiniBatchKMeans(**kmeans_args).fit_predict(model.wv.vectors)\n",
    "pickle.dump(clustering, open('./minibatchkmeans.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finished clustering'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"7a2a91c0-b2bc-460a-92d1-43628e5f0832\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"7a2a91c0-b2bc-460a-92d1-43628e5f0832\") === null) {\n",
       "                var notificationPayload = {\"icon\": \"/static/base/images/favicon.ico\", \"body\": \"finished clustering\", \"requireInteraction\": false};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -o\n",
    "msg = 'finished clustering'\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2centroid = {k: v for k, v in zip(model.wv.index2word, clustering)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "def make_bag_of_centroids(X, word_centroid_map, cluster_size):\n",
    "    centroids_list = []\n",
    "    for sentence in X:\n",
    "        centroids = zeros(cluster_size, dtype=\"float32\")\n",
    "        for word in sentence:\n",
    "            if word in word_centroid_map:\n",
    "                centroids[word_centroid_map[word]] += 1\n",
    "        centroids_list.append(centroids)\n",
    "    return centroids_list\n",
    "\n",
    "as_centroid = lambda s: make_bag_of_centroids(s, word2centroid, kmeans_args['n_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "\n",
    "fit = XGBClassifier().fit(scale(as_centroid(X_train)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koogleschreiber/.local/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning:\n",
      "\n",
      "The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = fit.predict(scale(as_centroid(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score predictions XGBClassifier:  0.2616822429906542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1=f1_score(y_predict, y_test)\n",
    "print(\"F1-Score predictions XGBClassifier: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "seed = 42\n",
    "k = 7\n",
    "jobs = -1\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best estimator...\n",
      "\n",
      "Mean accuracy Linear SVM: 0.673 (+/- 0.044)\n",
      "Mean accuracy SGDClassifier: 0.664 (+/- 0.037)\n",
      "Mean accuracy BernoulliNB: 0.701 (+/- 0.035)\n",
      "Mean accuracy LogisticRegression: 0.671 (+/- 0.039)\n",
      "Mean accuracy KNeighborsClassifier: 0.656 (+/- 0.020)\n",
      "Mean accuracy AdaBoostClassifier: 0.683 (+/- 0.032)\n",
      "Mean accuracy Random Forest: 0.666 (+/- 0.029)\n",
      "Mean accuracy Decision Tree: 0.636 (+/- 0.018)\n",
      "\n",
      "Best estimator: BernoulliNB (mean acc 0.701, 7-fold cross-validation)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "names = [\"Linear SVM\", \"SGDClassifier\", \"BernoulliNB\", \"LogisticRegression\",\n",
    "         \"KNeighborsClassifier\", \"AdaBoostClassifier\", \"Random Forest\", \"Decision Tree\"]\n",
    "\n",
    "classifiers = [\n",
    "    LinearSVC(random_state=seed),\n",
    "    SGDClassifier(max_iter=1000, tol=None),\n",
    "    BernoulliNB(),\n",
    "    LogisticRegression(random_state=seed, solver='sag', max_iter=1000),\n",
    "    KNeighborsClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    RandomForestClassifier(random_state=seed),\n",
    "    DecisionTreeClassifier(random_state=seed)\n",
    "]\n",
    "\n",
    "print('Searching best estimator...')\n",
    "print()\n",
    "best_classifier = None\n",
    "for name, clf in zip(names, classifiers):\n",
    "    scores = cross_val_score(clf, scale(as_centroid(X_train)), y_train, cv=k, n_jobs=jobs)\n",
    "    print('Mean accuracy %s: %0.3f (+/- %0.3f)' % (name, scores.mean(), scores.std() * 2))\n",
    "    if not best_classifier:\n",
    "        best_classifier = (name, scores.mean())\n",
    "    else:\n",
    "        if best_classifier[1] < scores.mean():\n",
    "            best_classifier = (name, scores.mean())\n",
    "print()\n",
    "print('Best estimator: %s (mean acc %0.3f, %d-fold cross-validation)' % (best_classifier[0], best_classifier[1], k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best estimator (F1 score) ...\n",
      "\n",
      "Mean F1 score Linear SVM: 0.481 (+/- 0.066)\n",
      "Mean F1 score SGDClassifier: 0.464 (+/- 0.069)\n",
      "Mean F1 score BernoulliNB: 0.490 (+/- 0.067)\n",
      "Mean F1 score LogisticRegression: 0.476 (+/- 0.057)\n",
      "Mean F1 score KNeighborsClassifier: 0.294 (+/- 0.070)\n",
      "Mean F1 score AdaBoostClassifier: 0.391 (+/- 0.086)\n",
      "Mean F1 score Random Forest: 0.340 (+/- 0.070)\n",
      "Mean F1 score Decision Tree: 0.451 (+/- 0.035)\n",
      "\n",
      "Best estimator: BernoulliNB (mean F1 score 0.490, 7-fold cross-validation)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "print('Searching best estimator (F1 score) ...')\n",
    "print()\n",
    "best_classifier = None\n",
    "for name, clf in zip(names, classifiers):\n",
    "    scores = cross_val_score(clf, scale(as_centroid(X_train)), y_train, cv=k, n_jobs=jobs, scoring='f1')\n",
    "    print('Mean F1 score %s: %0.3f (+/- %0.3f)' % (name, scores.mean(), scores.std() * 2))\n",
    "    if not best_classifier:\n",
    "        best_classifier = (name, scores.mean())\n",
    "    else:\n",
    "        if best_classifier[1] < scores.mean():\n",
    "            best_classifier = (name, scores.mean())\n",
    "print()\n",
    "print('Best estimator: %s (mean F1 score %0.3f, %d-fold cross-validation)' % (best_classifier[0], best_classifier[1], k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
