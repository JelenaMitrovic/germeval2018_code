{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### import libraries ###########################################\n",
    "%load_ext jupyternotify\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset = pd.DataFrame(columns=['X', 'y1', 'y2'])\n",
    "    #print('Loading dataset...')\n",
    "    with codecs.open(path, \"r\", encoding='utf-8', errors='ignore') as fdata:\n",
    "        for line in tqdm(fdata.readlines()):\n",
    "            line_split = line.split()\n",
    "            formated = ' '.join(line_split[:-2])\n",
    "            dataset.loc[-1] = [formated, line_split[-2], line_split[-1]]  # adding a row\n",
    "            dataset.index = dataset.index + 1  # shifting index\n",
    "            dataset = dataset.sort_index()  # sorting by index\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_txt(filename, X_test, y_pred):\n",
    "    i=0\n",
    "    results = []\n",
    "    for x in X_test:\n",
    "        line = []\n",
    "        line.append(x)\n",
    "        if y_pred[i] == 1:\n",
    "            line.append('OFFENSE')\n",
    "        else:\n",
    "            line.append('OTHER')\n",
    "        line.append('OTHER')\n",
    "        i += 1\n",
    "        results.append(line)\n",
    "\n",
    "    predictions = results\n",
    "    np.savetxt(filename, predictions, fmt=['%s', '%s', '%s'], delimiter='\\t', newline='\\n', \\\n",
    "                  header='', footer='', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "\n",
    "usernamePattern = re.compile('@[A-Za-z0-9_]{1,15}')\n",
    "urlPattern = re.compile('(https?:\\/\\/)[\\/.:\\w(1-9)]*\\s?')\n",
    "lbrPattern = re.compile('|LBR|')\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "tkz = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(usernamePattern, \"\",  text)\n",
    "    text = re.sub(urlPattern, \"\",  text)\n",
    "    text = re.sub(lbrPattern, \"\",  text)\n",
    "    output = []\n",
    "    tokens = tkz.tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_german:\n",
    "            if len(token) > 1:\n",
    "                if token[0] == '#':\n",
    "                    token = token[1:]\n",
    "                output.append(token)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tfidf(text):\n",
    "    text = re.sub(usernamePattern, \"\",  text)\n",
    "    text = re.sub(urlPattern, \"\",  text)\n",
    "    text = re.sub(lbrPattern, \"\",  text)\n",
    "    output = []\n",
    "    tokens = tkz.tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_german:\n",
    "            if len(token) > 1:\n",
    "                if token[0] == '#':\n",
    "                    token = token[1:]\n",
    "                output.append(stemmer.stem(token))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def so_load_data(path):\n",
    "    with open(path) as f:\n",
    "        content = f.read().splitlines()\n",
    "    # you may also want to remove whitespace characters like \\n at the end of each line\n",
    "    return [x.rstrip('\\\\n') for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5009/5009 [00:14<00:00, 339.90it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(path = '/home/text_mining_project/text_mining_project_2018/evaluation/germeval2018.training.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_dataset = so_load_data(path = '/home/text_mining_project/germeval2018.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_full =  pd.concat([dataset, dataset_own])\n",
    "dataset_full =  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "X_train_dataset = dataset_full['X'].values\n",
    "y_train = dataset_full['y1'].values\n",
    "\n",
    "X_test_raw = X_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(y):\n",
    "    np.ones(len(y))\n",
    "    choose = lambda l : 1 if l == 'OFFENSE' else 0\n",
    "    return [choose(l) for l in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = encode_label(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming documents...\n",
      "Transformation finished!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features = None\n",
    "stopwords_german = set(stopwords.words('german'))\n",
    "# stopwords_german = set(stopwords.words('english'))\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_tfidf, stop_words=stopwords_german, max_features=max_features, ngram_range=(1,3))\n",
    "print('Transforming documents...')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_dataset)\n",
    "X_test_tfidf = tfidf.transform(X_test_dataset)\n",
    "print('Transformation finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec_nostem_stopword.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "def make_bag_of_centroids(X, word_centroid_map, cluster_size):\n",
    "    centroids_list = []\n",
    "    for sentence in X:\n",
    "        centroids = zeros(cluster_size, dtype=\"float32\")\n",
    "        for word in sentence:\n",
    "            if word in word_centroid_map:\n",
    "                centroids[word_centroid_map[word]] += 1\n",
    "        centroids_list.append(centroids)\n",
    "    return centroids_list\n",
    "\n",
    "as_centroid = lambda s: make_bag_of_centroids(s, word2centroid, kmeans_args['n_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koogleschreiber/.local/lib/python3.5/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning:\n",
      "\n",
      "init_size=300 should be larger than k=1000. Setting it to 3*k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans_args = {\n",
    "    'n_clusters': 1000,\n",
    "}\n",
    "\n",
    "clustering = MiniBatchKMeans(**kmeans_args).fit_predict(model.wv.vectors)\n",
    "pickle.dump(clustering, open('./minibatchkmeans.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2centroid = {k: v for k, v in zip(model.wv.index2word, clustering)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vector_lists(x1, x2):\n",
    "    result = []\n",
    "    for i in range(len(x1)):\n",
    "        result.append(x1[i] + x2[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_german = set(stopwords.words('german'))\n",
    "# stopwords_german = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(X):\n",
    "    return [tokenize(sentence) for sentence in X]\n",
    "\n",
    "X_preprocess_train = preprocess(X_train_dataset)\n",
    "X_preprocess_test = preprocess(X_test_dataset)\n",
    "\n",
    "X_train_centroid = scale(as_centroid(X_preprocess_train))\n",
    "X_test_centroid = scale(as_centroid(X_preprocess_test))\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "X_train_centroid_sparse = csr_matrix(X_train_centroid)\n",
    "X_test_centroid_sparse = csr_matrix(X_test_centroid)\n",
    "\n",
    "\n",
    "X_train = hstack([X_train_centroid_sparse, X_train_tfidf])\n",
    "X_test = hstack([X_test_centroid_sparse, X_test_tfidf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lsvc = LinearSVC(penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_train = model.transform(X_train)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2\n",
    "# \n",
    "# scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "# X_train = scaler.fit_transform(X_train.todense())\n",
    "# X_test = scaler.transform(X_test.todense())\n",
    "# \n",
    "# ch2 = SelectKBest(chi2, k=2000)\n",
    "# X_train = ch2.fit_transform(X_train, y_train)\n",
    "# X_test = ch2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "seed = 42\n",
    "k = 7\n",
    "jobs = -1\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB(alpha=0.1, binarize=0, class_prior=None, fit_prior=False)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_txt('upInf_coarse_2.txt', X_test_raw, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
